{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico/Desktop/NLP-project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-01 14:19:55.945474: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 14:19:55.951779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733059195.960013    8391 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733059195.962702    8391 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 14:19:55.972834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento e pre-elaborazione del dataset...\n",
      "Creazione del DataLoader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733059200.093156    8391 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5806 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch di input: (32, 512)\n",
      "Batch di output: (32, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 14:20:00.570475: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "from data.preprocess import preprocess_cnn_dailymail\n",
    "from utils.helpers import create_tf_dataloader\n",
    "\n",
    "# Pre-elabora i dati\n",
    "print(\"Caricamento e pre-elaborazione del dataset...\")\n",
    "tokenized_dataset, tokenizer = preprocess_cnn_dailymail(fraction=0.25)\n",
    "\n",
    "# Crea il DataLoader\n",
    "print(\"Creazione del DataLoader...\")\n",
    "train_loader = create_tf_dataloader(tokenized_dataset['train'])\n",
    "\n",
    "# Verifica il DataLoader\n",
    "for batch in train_loader.take(1):\n",
    "    print(\"Batch di input:\", batch[0][\"input_ids\"].shape)\n",
    "    print(\"Batch di output:\", batch[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento e pre-elaborazione del dataset...\n",
      "Addestramento del modello...\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733059215.729282    8509 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss = 10.377904891967773\n",
      "Batch 10: Loss = 8.090767860412598\n",
      "Batch 20: Loss = 4.532679557800293\n",
      "Batch 30: Loss = 3.6838343143463135\n",
      "Batch 40: Loss = 3.5577125549316406\n",
      "Batch 50: Loss = 3.3847694396972656\n",
      "Batch 60: Loss = 3.3483638763427734\n",
      "Batch 70: Loss = 3.2901644706726074\n",
      "Batch 80: Loss = 3.377471685409546\n",
      "Batch 90: Loss = 3.334153175354004\n",
      "Batch 100: Loss = 3.212480306625366\n",
      "Batch 110: Loss = 3.1018786430358887\n",
      "Batch 120: Loss = 3.09779953956604\n",
      "Batch 130: Loss = 3.203455924987793\n",
      "Batch 140: Loss = 3.0093994140625\n",
      "Batch 150: Loss = 3.0632143020629883\n",
      "Batch 160: Loss = 2.9749999046325684\n",
      "Batch 170: Loss = 3.0274815559387207\n",
      "Batch 180: Loss = 3.0876028537750244\n",
      "Batch 190: Loss = 3.177318572998047\n",
      "Batch 200: Loss = 2.9220712184906006\n",
      "Batch 210: Loss = 3.0446488857269287\n",
      "Batch 220: Loss = 3.1545519828796387\n",
      "Batch 230: Loss = 2.955371618270874\n",
      "Batch 240: Loss = 3.155029773712158\n",
      "Batch 250: Loss = 2.9014151096343994\n",
      "Batch 260: Loss = 3.0543932914733887\n",
      "Batch 270: Loss = 3.010183334350586\n",
      "Batch 280: Loss = 3.069173812866211\n",
      "Batch 290: Loss = 2.905924081802368\n",
      "Batch 300: Loss = 3.0503711700439453\n",
      "Batch 310: Loss = 2.9156017303466797\n",
      "Batch 320: Loss = 3.14123272895813\n",
      "Batch 330: Loss = 2.939129114151001\n",
      "Batch 340: Loss = 2.892971992492676\n",
      "Batch 350: Loss = 2.715632200241089\n",
      "Batch 360: Loss = 3.0183520317077637\n",
      "Batch 370: Loss = 3.0560312271118164\n",
      "Batch 380: Loss = 2.7399299144744873\n",
      "Batch 390: Loss = 2.621393918991089\n",
      "Batch 400: Loss = 2.7545201778411865\n",
      "Batch 410: Loss = 2.7596700191497803\n",
      "Batch 420: Loss = 2.77683424949646\n",
      "Batch 430: Loss = 2.7421376705169678\n",
      "Batch 440: Loss = 2.5375735759735107\n",
      "Batch 450: Loss = 2.7386298179626465\n",
      "Batch 460: Loss = 2.763828754425049\n",
      "Batch 470: Loss = 2.7220499515533447\n",
      "Batch 480: Loss = 2.684197187423706\n",
      "Batch 490: Loss = 2.7881579399108887\n",
      "Batch 500: Loss = 2.7734341621398926\n",
      "Batch 510: Loss = 2.431570053100586\n",
      "Batch 520: Loss = 2.7022511959075928\n",
      "Batch 530: Loss = 2.838468313217163\n",
      "Batch 540: Loss = 2.6781399250030518\n",
      "Batch 550: Loss = 2.7903263568878174\n",
      "Batch 560: Loss = 2.569284677505493\n",
      "Batch 570: Loss = 2.724867820739746\n",
      "Batch 580: Loss = 2.87197208404541\n",
      "Batch 590: Loss = 2.6126086711883545\n",
      "Batch 600: Loss = 2.7287704944610596\n",
      "Batch 610: Loss = 2.635106086730957\n",
      "Batch 620: Loss = 2.6509785652160645\n",
      "Batch 630: Loss = 2.5306360721588135\n",
      "Batch 640: Loss = 2.4704031944274902\n",
      "Batch 650: Loss = 2.830015182495117\n",
      "Batch 660: Loss = 2.669844150543213\n",
      "Batch 670: Loss = 2.5469155311584473\n",
      "Batch 680: Loss = 2.540238380432129\n",
      "Batch 690: Loss = 2.4598982334136963\n",
      "Batch 700: Loss = 2.8560070991516113\n",
      "Batch 710: Loss = 2.7944717407226562\n",
      "Batch 720: Loss = 2.7124745845794678\n",
      "Batch 730: Loss = 2.7061402797698975\n",
      "Batch 740: Loss = 2.6789236068725586\n",
      "Batch 750: Loss = 2.6396255493164062\n",
      "Batch 760: Loss = 2.4144113063812256\n",
      "Batch 770: Loss = 2.6692934036254883\n",
      "Batch 780: Loss = 2.703357458114624\n",
      "Batch 790: Loss = 2.8062992095947266\n",
      "Batch 800: Loss = 2.904346466064453\n",
      "Batch 810: Loss = 2.8043668270111084\n",
      "Batch 820: Loss = 2.8620383739471436\n",
      "Batch 830: Loss = 2.7659924030303955\n",
      "Batch 840: Loss = 2.9825856685638428\n",
      "Batch 850: Loss = 2.798841953277588\n",
      "Batch 860: Loss = 2.779956817626953\n",
      "Batch 870: Loss = 2.733750343322754\n",
      "Batch 880: Loss = 2.944730758666992\n",
      "Batch 890: Loss = 2.558544635772705\n",
      "Batch 900: Loss = 2.607748508453369\n",
      "Batch 910: Loss = 2.5140187740325928\n",
      "Batch 920: Loss = 2.705414295196533\n",
      "Batch 930: Loss = 2.6762471199035645\n",
      "Batch 940: Loss = 2.6631133556365967\n",
      "Batch 950: Loss = 2.824333429336548\n",
      "Batch 960: Loss = 2.648149013519287\n",
      "Batch 970: Loss = 2.5620999336242676\n",
      "Batch 980: Loss = 2.3913631439208984\n",
      "Batch 990: Loss = 2.8282859325408936\n",
      "Batch 1000: Loss = 2.6329240798950195\n",
      "Batch 1010: Loss = 2.695051431655884\n",
      "Batch 1020: Loss = 2.3547966480255127\n",
      "Batch 1030: Loss = 2.5070512294769287\n",
      "Batch 1040: Loss = 2.50734806060791\n",
      "Batch 1050: Loss = 2.5376248359680176\n",
      "Batch 1060: Loss = 2.465956211090088\n",
      "Batch 1070: Loss = 2.7951745986938477\n",
      "Batch 1080: Loss = 2.5747690200805664\n",
      "Batch 1090: Loss = 2.4643523693084717\n",
      "Batch 1100: Loss = 2.8151814937591553\n",
      "Batch 1110: Loss = 2.6773316860198975\n",
      "Batch 1120: Loss = 2.6660192012786865\n",
      "Batch 1130: Loss = 2.5853660106658936\n",
      "Batch 1140: Loss = 2.7421064376831055\n",
      "Batch 1150: Loss = 2.866288900375366\n",
      "Batch 1160: Loss = 2.7240419387817383\n",
      "Batch 1170: Loss = 2.4924309253692627\n",
      "Batch 1180: Loss = 2.6388514041900635\n",
      "Batch 1190: Loss = 2.6527411937713623\n",
      "Batch 1200: Loss = 2.6779632568359375\n",
      "Batch 1210: Loss = 2.4362876415252686\n",
      "Batch 1220: Loss = 2.75235652923584\n",
      "Batch 1230: Loss = 2.510556221008301\n",
      "Batch 1240: Loss = 2.5624337196350098\n",
      "Batch 1250: Loss = 2.600762128829956\n",
      "Batch 1260: Loss = 2.462155342102051\n",
      "Batch 1270: Loss = 2.7522201538085938\n",
      "Batch 1280: Loss = 2.425028085708618\n",
      "Batch 1290: Loss = 2.400134325027466\n",
      "Batch 1300: Loss = 2.549694538116455\n",
      "Batch 1310: Loss = 2.6385538578033447\n",
      "Batch 1320: Loss = 2.652972459793091\n",
      "Batch 1330: Loss = 2.6026384830474854\n",
      "Batch 1340: Loss = 2.5435431003570557\n",
      "Batch 1350: Loss = 2.4649555683135986\n",
      "Batch 1360: Loss = 2.5863282680511475\n",
      "Batch 1370: Loss = 2.5969133377075195\n",
      "Batch 1380: Loss = 2.5182273387908936\n",
      "Batch 1390: Loss = 2.494804859161377\n",
      "Batch 1400: Loss = 2.597332239151001\n",
      "Batch 1410: Loss = 2.628631830215454\n",
      "Batch 1420: Loss = 2.5720865726470947\n",
      "Batch 1430: Loss = 2.5367531776428223\n",
      "Batch 1440: Loss = 2.6374499797821045\n",
      "Batch 1450: Loss = 2.647512197494507\n",
      "Batch 1460: Loss = 2.621389389038086\n",
      "Batch 1470: Loss = 2.5559446811676025\n",
      "Batch 1480: Loss = 2.2613778114318848\n",
      "Batch 1490: Loss = 2.4985039234161377\n",
      "Batch 1500: Loss = 2.5229427814483643\n",
      "Batch 1510: Loss = 2.550398111343384\n",
      "Batch 1520: Loss = 2.3735737800598145\n",
      "Batch 1530: Loss = 2.53973388671875\n",
      "Batch 1540: Loss = 2.6349692344665527\n",
      "Batch 1550: Loss = 2.517836093902588\n",
      "Batch 1560: Loss = 2.4116358757019043\n",
      "Batch 1570: Loss = 2.479393720626831\n",
      "Batch 1580: Loss = 2.587163209915161\n",
      "Batch 1590: Loss = 2.4667763710021973\n",
      "Batch 1600: Loss = 2.588118076324463\n",
      "Batch 1610: Loss = 2.5719830989837646\n",
      "Batch 1620: Loss = 2.5381386280059814\n",
      "Batch 1630: Loss = 2.497265100479126\n",
      "Batch 1640: Loss = 2.5553479194641113\n",
      "Batch 1650: Loss = 2.7326607704162598\n",
      "Batch 1660: Loss = 2.5524840354919434\n",
      "Batch 1670: Loss = 2.558657646179199\n",
      "Batch 1680: Loss = 2.3853750228881836\n",
      "Batch 1690: Loss = 2.522406816482544\n",
      "Batch 1700: Loss = 2.4441683292388916\n",
      "Batch 1710: Loss = 2.5209763050079346\n",
      "Batch 1720: Loss = 2.3523552417755127\n",
      "Batch 1730: Loss = 2.4057438373565674\n",
      "Batch 1740: Loss = 2.444352865219116\n",
      "Batch 1750: Loss = 2.287280559539795\n",
      "Batch 1760: Loss = 2.330449342727661\n",
      "Batch 1770: Loss = 2.6198651790618896\n",
      "Batch 1780: Loss = 2.4266204833984375\n",
      "Batch 1790: Loss = 2.259181499481201\n",
      "Batch 1800: Loss = 2.3228237628936768\n",
      "Batch 1810: Loss = 2.4093129634857178\n",
      "Batch 1820: Loss = 2.582888126373291\n",
      "Batch 1830: Loss = 2.4930176734924316\n",
      "Batch 1840: Loss = 2.453740358352661\n",
      "Batch 1850: Loss = 2.4320051670074463\n",
      "Batch 1860: Loss = 2.633150100708008\n",
      "Batch 1870: Loss = 2.4338197708129883\n",
      "Batch 1880: Loss = 2.3442132472991943\n",
      "Batch 1890: Loss = 2.3004608154296875\n",
      "Batch 1900: Loss = 2.3491714000701904\n",
      "Batch 1910: Loss = 2.4382362365722656\n",
      "Batch 1920: Loss = 2.310206413269043\n",
      "Batch 1930: Loss = 2.356977939605713\n",
      "Batch 1940: Loss = 2.3406789302825928\n",
      "Batch 1950: Loss = 2.4890999794006348\n",
      "Batch 1960: Loss = 2.4538230895996094\n",
      "Batch 1970: Loss = 2.608746290206909\n",
      "Batch 1980: Loss = 2.624284267425537\n",
      "Batch 1990: Loss = 2.3410632610321045\n",
      "Batch 2000: Loss = 2.401951789855957\n",
      "Batch 2010: Loss = 2.346712112426758\n",
      "Batch 2020: Loss = 2.3062093257904053\n",
      "Batch 2030: Loss = 2.4928765296936035\n",
      "Batch 2040: Loss = 2.3955459594726562\n",
      "Batch 2050: Loss = 2.5328006744384766\n",
      "Batch 2060: Loss = 2.517190933227539\n",
      "Batch 2070: Loss = 2.3406925201416016\n",
      "Batch 2080: Loss = 2.340805768966675\n",
      "Batch 2090: Loss = 2.4580581188201904\n",
      "Batch 2100: Loss = 2.43645977973938\n",
      "Batch 2110: Loss = 2.476442813873291\n",
      "Batch 2120: Loss = 2.411607265472412\n",
      "Batch 2130: Loss = 2.4364688396453857\n",
      "Batch 2140: Loss = 2.264439105987549\n",
      "Batch 2150: Loss = 2.360747814178467\n",
      "Batch 2160: Loss = 2.47149658203125\n",
      "Batch 2170: Loss = 2.208249807357788\n",
      "Batch 2180: Loss = 2.4124696254730225\n",
      "Batch 2190: Loss = 2.3473258018493652\n",
      "Batch 2200: Loss = 2.249238967895508\n",
      "Batch 2210: Loss = 2.439084529876709\n",
      "Batch 2220: Loss = 2.1960225105285645\n",
      "Batch 2230: Loss = 2.4599034786224365\n",
      "Batch 2240: Loss = 2.4797251224517822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 14:25:08.268714: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "Batch 0: Loss = 2.236063241958618\n",
      "Batch 10: Loss = 2.2483675479888916\n",
      "Batch 20: Loss = 2.378700017929077\n",
      "Batch 30: Loss = 2.3297810554504395\n",
      "Batch 40: Loss = 2.3361833095550537\n",
      "Batch 50: Loss = 2.4048514366149902\n",
      "Batch 60: Loss = 2.22408127784729\n",
      "Batch 70: Loss = 2.297367811203003\n",
      "Batch 80: Loss = 2.365776777267456\n",
      "Batch 90: Loss = 2.2490482330322266\n",
      "Batch 100: Loss = 2.2799994945526123\n",
      "Batch 110: Loss = 2.316436529159546\n",
      "Batch 120: Loss = 2.3836448192596436\n",
      "Batch 130: Loss = 2.545253276824951\n",
      "Batch 140: Loss = 2.3712387084960938\n",
      "Batch 150: Loss = 2.331066131591797\n",
      "Batch 160: Loss = 2.346876382827759\n",
      "Batch 170: Loss = 2.389976739883423\n",
      "Batch 180: Loss = 2.2138285636901855\n",
      "Batch 190: Loss = 2.399153470993042\n",
      "Batch 200: Loss = 2.440418004989624\n",
      "Batch 210: Loss = 2.382948398590088\n",
      "Batch 220: Loss = 2.349318027496338\n",
      "Batch 230: Loss = 2.3951833248138428\n",
      "Batch 240: Loss = 2.2941064834594727\n",
      "Batch 250: Loss = 2.5314462184906006\n",
      "Batch 260: Loss = 2.3153512477874756\n",
      "Batch 270: Loss = 2.3729088306427\n",
      "Batch 280: Loss = 2.5149173736572266\n",
      "Batch 290: Loss = 2.4457340240478516\n",
      "Batch 300: Loss = 2.541386127471924\n",
      "Batch 310: Loss = 2.3691253662109375\n",
      "Batch 320: Loss = 2.3181307315826416\n",
      "Batch 330: Loss = 2.4425039291381836\n",
      "Batch 340: Loss = 2.3385281562805176\n",
      "Batch 350: Loss = 2.4417483806610107\n",
      "Batch 360: Loss = 2.1755435466766357\n",
      "Batch 370: Loss = 2.2023067474365234\n",
      "Batch 380: Loss = 2.159740924835205\n",
      "Batch 390: Loss = 2.3554327487945557\n",
      "Batch 400: Loss = 2.294569492340088\n",
      "Batch 410: Loss = 2.2039413452148438\n",
      "Batch 420: Loss = 2.172973394393921\n",
      "Batch 430: Loss = 1.9370707273483276\n",
      "Batch 440: Loss = 2.187005043029785\n",
      "Batch 450: Loss = 2.0659375190734863\n",
      "Batch 460: Loss = 2.3621039390563965\n",
      "Batch 470: Loss = 2.159698724746704\n",
      "Batch 480: Loss = 2.2546353340148926\n",
      "Batch 490: Loss = 2.128571033477783\n",
      "Batch 500: Loss = 2.188899278640747\n",
      "Batch 510: Loss = 2.1539146900177\n",
      "Batch 520: Loss = 2.0402297973632812\n",
      "Batch 530: Loss = 2.0282235145568848\n",
      "Batch 540: Loss = 2.2711713314056396\n",
      "Batch 550: Loss = 2.2600979804992676\n",
      "Batch 560: Loss = 2.1059131622314453\n",
      "Batch 570: Loss = 2.0712027549743652\n",
      "Batch 580: Loss = 2.016409397125244\n",
      "Batch 590: Loss = 2.178617477416992\n",
      "Batch 600: Loss = 1.9981504678726196\n",
      "Batch 610: Loss = 2.2220957279205322\n",
      "Batch 620: Loss = 2.147738456726074\n",
      "Batch 630: Loss = 2.2767083644866943\n",
      "Batch 640: Loss = 2.132854700088501\n",
      "Batch 650: Loss = 2.1769490242004395\n",
      "Batch 660: Loss = 1.9668691158294678\n",
      "Batch 670: Loss = 2.268903970718384\n",
      "Batch 680: Loss = 2.301725387573242\n",
      "Batch 690: Loss = 2.010016918182373\n",
      "Batch 700: Loss = 2.245232343673706\n",
      "Batch 710: Loss = 2.372893810272217\n",
      "Batch 720: Loss = 2.249405860900879\n",
      "Batch 730: Loss = 2.1086061000823975\n",
      "Batch 740: Loss = 2.4361753463745117\n",
      "Batch 750: Loss = 2.2040534019470215\n",
      "Batch 760: Loss = 2.1326141357421875\n",
      "Batch 770: Loss = 2.2134017944335938\n",
      "Batch 780: Loss = 2.3596608638763428\n",
      "Batch 790: Loss = 2.1928555965423584\n",
      "Batch 800: Loss = 2.116070508956909\n",
      "Batch 810: Loss = 2.1780776977539062\n",
      "Batch 820: Loss = 2.1426589488983154\n",
      "Batch 830: Loss = 2.3583645820617676\n",
      "Batch 840: Loss = 2.238151788711548\n",
      "Batch 850: Loss = 2.1568450927734375\n",
      "Batch 860: Loss = 2.3035197257995605\n",
      "Batch 870: Loss = 2.3374905586242676\n",
      "Batch 880: Loss = 2.2951385974884033\n",
      "Batch 890: Loss = 2.161118984222412\n",
      "Batch 900: Loss = 2.3990085124969482\n",
      "Batch 910: Loss = 2.2242040634155273\n",
      "Batch 920: Loss = 2.2936630249023438\n",
      "Batch 930: Loss = 2.5652225017547607\n",
      "Batch 940: Loss = 2.225606918334961\n",
      "Batch 950: Loss = 2.2528018951416016\n",
      "Batch 960: Loss = 2.2326245307922363\n",
      "Batch 970: Loss = 2.223361015319824\n",
      "Batch 980: Loss = 2.1451520919799805\n",
      "Batch 990: Loss = 2.187420129776001\n",
      "Batch 1000: Loss = 2.1942524909973145\n",
      "Batch 1010: Loss = 2.2568743228912354\n",
      "Batch 1020: Loss = 2.362957000732422\n",
      "Batch 1030: Loss = 2.1861207485198975\n",
      "Batch 1040: Loss = 2.1407852172851562\n",
      "Batch 1050: Loss = 2.346609115600586\n",
      "Batch 1060: Loss = 2.0381674766540527\n",
      "Batch 1070: Loss = 2.1139793395996094\n",
      "Batch 1080: Loss = 2.130899667739868\n",
      "Batch 1090: Loss = 2.206521987915039\n",
      "Batch 1100: Loss = 2.216782569885254\n",
      "Batch 1110: Loss = 2.2673797607421875\n",
      "Batch 1120: Loss = 2.2626469135284424\n",
      "Batch 1130: Loss = 2.282987356185913\n",
      "Batch 1140: Loss = 2.3415417671203613\n",
      "Batch 1150: Loss = 2.149782180786133\n",
      "Batch 1160: Loss = 2.2508320808410645\n",
      "Batch 1170: Loss = 2.306151866912842\n",
      "Batch 1180: Loss = 2.2546679973602295\n",
      "Batch 1190: Loss = 2.2757456302642822\n",
      "Batch 1200: Loss = 2.454129934310913\n",
      "Batch 1210: Loss = 2.0552783012390137\n",
      "Batch 1220: Loss = 2.234365463256836\n",
      "Batch 1230: Loss = 2.2563421726226807\n",
      "Batch 1240: Loss = 2.2684202194213867\n",
      "Batch 1250: Loss = 2.317368984222412\n",
      "Batch 1260: Loss = 2.2191271781921387\n",
      "Batch 1270: Loss = 2.3017165660858154\n",
      "Batch 1280: Loss = 2.1999356746673584\n",
      "Batch 1290: Loss = 2.067286968231201\n",
      "Batch 1300: Loss = 2.3495688438415527\n",
      "Batch 1310: Loss = 2.0615620613098145\n",
      "Batch 1320: Loss = 2.104238510131836\n",
      "Batch 1330: Loss = 2.159383535385132\n",
      "Batch 1340: Loss = 2.172419309616089\n",
      "Batch 1350: Loss = 2.1440649032592773\n",
      "Batch 1360: Loss = 2.151142120361328\n",
      "Batch 1370: Loss = 2.1774098873138428\n",
      "Batch 1380: Loss = 2.2087368965148926\n",
      "Batch 1390: Loss = 2.1539268493652344\n",
      "Batch 1400: Loss = 2.3271124362945557\n",
      "Batch 1410: Loss = 2.051816701889038\n",
      "Batch 1420: Loss = 2.2746365070343018\n",
      "Batch 1430: Loss = 2.251884698867798\n",
      "Batch 1440: Loss = 2.048229694366455\n",
      "Batch 1450: Loss = 2.223595142364502\n",
      "Batch 1460: Loss = 2.237762928009033\n",
      "Batch 1470: Loss = 2.301835536956787\n",
      "Batch 1480: Loss = 1.9783579111099243\n",
      "Batch 1490: Loss = 2.055448532104492\n",
      "Batch 1500: Loss = 2.168001174926758\n",
      "Batch 1510: Loss = 2.0553817749023438\n",
      "Batch 1520: Loss = 2.101212739944458\n",
      "Batch 1530: Loss = 2.1957499980926514\n",
      "Batch 1540: Loss = 2.1818253993988037\n",
      "Batch 1550: Loss = 2.249290704727173\n",
      "Batch 1560: Loss = 2.0060842037200928\n",
      "Batch 1570: Loss = 2.2278783321380615\n",
      "Batch 1580: Loss = 2.265381336212158\n",
      "Batch 1590: Loss = 2.14501953125\n",
      "Batch 1600: Loss = 2.309480667114258\n",
      "Batch 1610: Loss = 2.1249120235443115\n",
      "Batch 1620: Loss = 2.2663650512695312\n",
      "Batch 1630: Loss = 2.289496898651123\n",
      "Batch 1640: Loss = 2.155458688735962\n",
      "Batch 1650: Loss = 2.2691502571105957\n",
      "Batch 1660: Loss = 2.2214438915252686\n",
      "Batch 1670: Loss = 2.0807623863220215\n",
      "Batch 1680: Loss = 2.418846368789673\n",
      "Batch 1690: Loss = 2.303189277648926\n",
      "Batch 1700: Loss = 2.188199043273926\n",
      "Batch 1710: Loss = 2.1407272815704346\n",
      "Batch 1720: Loss = 2.1270546913146973\n",
      "Batch 1730: Loss = 2.2998299598693848\n",
      "Batch 1740: Loss = 2.165445327758789\n",
      "Batch 1750: Loss = 2.1120212078094482\n",
      "Batch 1760: Loss = 2.0516622066497803\n",
      "Batch 1770: Loss = 2.2651185989379883\n",
      "Batch 1780: Loss = 2.1005606651306152\n",
      "Batch 1790: Loss = 2.1371378898620605\n",
      "Batch 1800: Loss = 2.1524767875671387\n",
      "Batch 1810: Loss = 2.2509002685546875\n",
      "Batch 1820: Loss = 2.2609498500823975\n",
      "Batch 1830: Loss = 2.1334872245788574\n",
      "Batch 1840: Loss = 2.096848249435425\n",
      "Batch 1850: Loss = 2.2050609588623047\n",
      "Batch 1860: Loss = 2.191718816757202\n",
      "Batch 1870: Loss = 2.2015140056610107\n",
      "Batch 1880: Loss = 2.113801956176758\n",
      "Batch 1890: Loss = 2.095855951309204\n",
      "Batch 1900: Loss = 1.9505891799926758\n",
      "Batch 1910: Loss = 2.1097657680511475\n",
      "Batch 1920: Loss = 2.137716770172119\n",
      "Batch 1930: Loss = 1.9505646228790283\n",
      "Batch 1940: Loss = 2.0160584449768066\n",
      "Batch 1950: Loss = 2.1758358478546143\n",
      "Batch 1960: Loss = 2.150930643081665\n",
      "Batch 1970: Loss = 2.18896222114563\n",
      "Batch 1980: Loss = 2.0364415645599365\n",
      "Batch 1990: Loss = 2.3352906703948975\n",
      "Batch 2000: Loss = 2.368950128555298\n",
      "Batch 2010: Loss = 2.299705743789673\n",
      "Batch 2020: Loss = 1.9678869247436523\n",
      "Batch 2030: Loss = 2.207242488861084\n",
      "Batch 2040: Loss = 2.2045774459838867\n",
      "Batch 2050: Loss = 2.172114133834839\n",
      "Batch 2060: Loss = 2.1346375942230225\n",
      "Batch 2070: Loss = 2.1908504962921143\n",
      "Batch 2080: Loss = 2.2708280086517334\n",
      "Batch 2090: Loss = 2.0374913215637207\n",
      "Batch 2100: Loss = 2.135873556137085\n",
      "Batch 2110: Loss = 2.154803514480591\n",
      "Batch 2120: Loss = 2.1010613441467285\n",
      "Batch 2130: Loss = 2.0810043811798096\n",
      "Batch 2140: Loss = 2.0125834941864014\n",
      "Batch 2150: Loss = 2.1665663719177246\n",
      "Batch 2160: Loss = 2.166940927505493\n",
      "Batch 2170: Loss = 2.031092405319214\n",
      "Batch 2180: Loss = 2.1909682750701904\n",
      "Batch 2190: Loss = 2.00488543510437\n",
      "Batch 2200: Loss = 2.1696181297302246\n",
      "Batch 2210: Loss = 2.2824926376342773\n",
      "Batch 2220: Loss = 2.1676621437072754\n",
      "Batch 2230: Loss = 2.172445774078369\n",
      "Batch 2240: Loss = 2.1192402839660645\n"
     ]
    }
   ],
   "source": [
    "from data.preprocess import preprocess_cnn_dailymail\n",
    "from utils.helpers import create_tf_dataloader\n",
    "from model.encoder_decoder import train_model\n",
    "from config import Config\n",
    "\n",
    "# Configura i parametri\n",
    "config = Config()\n",
    "\n",
    "# Pre-elabora i dati\n",
    "print(\"Caricamento e pre-elaborazione del dataset...\")\n",
    "tokenized_dataset, tokenizer = preprocess_cnn_dailymail(fraction=0.25)\n",
    "train_loader = create_tf_dataloader(tokenized_dataset['train'], batch_size=config.BATCH_SIZE)\n",
    "\n",
    "# Addestra il modello\n",
    "print(\"Addestramento del modello...\")\n",
    "train_model(\n",
    "    train_loader,\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    embedding_dim=config.EMBEDDING_DIM,\n",
    "    hidden_dim=config.HIDDEN_DIM,\n",
    "    epochs=config.EPOCHS,\n",
    "    learning_rate=config.LEARNING_RATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvataggio modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"saves/E{config.EPOCHS}-B{config.BATCH_SIZE}-L{config.LEARNING_RATE}\"\n",
    "model.save_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m test_dataset:\n\u001b[1;32m     20\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m---> 21\u001b[0m     generated_summary \u001b[38;5;241m=\u001b[39m generate_summary(\u001b[43mmodel\u001b[49m, input_ids)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mdecode(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mskip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Summary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "test_loader = create_tf_dataloader(test_dataset, batch_size=config.BATCH_SIZE)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Carica il tokenizer usato durante il training\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Funzione per generare riassunti\n",
    "def generate_summary(model, input_ids, max_length=150):\n",
    "    # Passa gli input_ids al modello per ottenere l'output del decoder\n",
    "    output_ids = model.predict(input_ids)\n",
    "    \n",
    "    # Decodifica i token generati\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "for sample in test_dataset:\n",
    "    input_ids = tf.convert_to_tensor([sample[\"input_ids\"]])\n",
    "    generated_summary = generate_summary(model, input_ids)\n",
    "    print(f\"Article: {tokenizer.decode(sample['input_ids'], skip_special_tokens=True)}\")\n",
    "    print(f\"Generated Summary: {generated_summary}\")\n",
    "    print(f\"Reference Summary: {sample['highlights']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Inizializza lo scorer ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calcola i punteggi\n",
    "def evaluate_model(model, test_dataset, tokenizer, max_length=150):\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    \n",
    "    for sample in test_dataset:\n",
    "        input_ids = tf.convert_to_tensor([sample[\"input_ids\"]])\n",
    "        generated_summary = generate_summary(model, input_ids, max_length)\n",
    "        reference_summary = sample[\"highlights\"]\n",
    "        \n",
    "        # Calcola i punteggi ROUGE per ogni esempio\n",
    "        scores = scorer.score(reference_summary, generated_summary)\n",
    "        for key in scores:\n",
    "            rouge_scores[key].append(scores[key].fmeasure)\n",
    "    \n",
    "    # Calcola le medie\n",
    "    avg_scores = {key: sum(scores) / len(scores) for key, scores in rouge_scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Esegui la valutazione\n",
    "scores = evaluate_model(model, test_dataset, tokenizer)\n",
    "print(f\"ROUGE scores: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Your article text here.\"\n",
    "inputs = tokenizer(article, max_length=512, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Genera il riassunto\n",
    "generated_summary = generate_summary(model, inputs[\"input_ids\"])\n",
    "print(f\"Generated Summary: {generated_summary}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
